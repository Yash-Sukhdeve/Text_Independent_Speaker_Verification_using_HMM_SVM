{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_arrays_dicts(dict_a, dict_b):\n",
    "    \"\"\"\n",
    "    Adds the numpy arrays from dict_b to the numpy arrays in dict_a for matching keys.\n",
    "    This function concatenates the array from dict_b (with 4 elements) to the array in dict_a (with 142 elements),\n",
    "    resulting in an array of 146 elements.\n",
    "\n",
    "    Parameters:\n",
    "    dict_a (dict): The first dictionary containing numpy arrays.\n",
    "    dict_b (dict): The second dictionary containing numpy arrays to add.\n",
    "\n",
    "    Returns:\n",
    "    dict: A new dictionary with the concatenated numpy arrays.\n",
    "    \"\"\"\n",
    "    # Ensure both dictionaries have the same keys\n",
    "    if dict_a.keys() != dict_b.keys():\n",
    "        raise ValueError(\"The dictionaries do not have the same keys.\")\n",
    "    \n",
    "    # Initialize an empty dictionary to store the results\n",
    "    result_dict = {}\n",
    "\n",
    "    # Iterate over each key\n",
    "    for key in dict_a:\n",
    "        # Ensure both dictionaries have lists of numpy arrays for the current key\n",
    "        if len(dict_a[key]) != len(dict_b[key]):\n",
    "            raise ValueError(f\"Length mismatch for key '{key}' in both dictionaries.\")\n",
    "        \n",
    "        # Concatenate corresponding numpy arrays from dict_a and dict_b\n",
    "        result_dict[key] = [np.concatenate((a, b)) for a, b in zip(dict_a[key], dict_b[key])]\n",
    "    \n",
    "    return result_dict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def add_last_n_elements(dict_a, dict_b, n):\n",
    "    \"\"\"\n",
    "    Adds the last 'n' elements of the numpy arrays from dict_b to the numpy arrays in dict_a for matching keys.\n",
    "    This function concatenates the last 'n' elements from dict_b (with 'n' elements) to the array in dict_a (with 'm' elements),\n",
    "    resulting in an array of 'm + n' elements.\n",
    "\n",
    "    Parameters:\n",
    "    dict_a (dict): The first dictionary containing numpy arrays.\n",
    "    dict_b (dict): The second dictionary containing numpy arrays to add.\n",
    "    n (int): The number of elements to add from each array in dict_b.\n",
    "\n",
    "    Returns:\n",
    "    dict: A new dictionary with the concatenated numpy arrays.\n",
    "    \"\"\"\n",
    "    # Ensure both dictionaries have the same keys\n",
    "    if dict_a.keys() != dict_b.keys():\n",
    "        raise ValueError(\"The dictionaries do not have the same keys.\")\n",
    "    \n",
    "    # Initialize an empty dictionary to store the results\n",
    "    result_dict = {}\n",
    "\n",
    "    # Iterate over each key\n",
    "    for key in dict_a:\n",
    "        # Ensure both dictionaries have lists of numpy arrays for the current key\n",
    "        if len(dict_a[key]) != len(dict_b[key]):\n",
    "            raise ValueError(f\"Length mismatch for key '{key}' in both dictionaries.\")\n",
    "        \n",
    "        # Concatenate the numpy array from dict_a with the last n elements from the corresponding array in dict_b\n",
    "        result_dict[key] = [\n",
    "            np.concatenate((a, b[-n:])) for a, b in zip(dict_a[key], dict_b[key])\n",
    "        ]\n",
    "    \n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original feature set\n",
      "    number of samples : 300\n",
      "    number of features: 142\n",
      "additional feature set\n",
      "    number of samples : 300\n",
      "    number of features: 4\n"
     ]
    }
   ],
   "source": [
    "original_set_path   = \"../../../data/extracted_features_v2/mfcc_20_features.pickle\"\n",
    "additional_set_path = \"../../../data/extracted_features_v2/format_frequencies.pickle\"\n",
    "\n",
    "with open(original_set_path, \"rb\") as file:\n",
    "    original_feature_set = pickle.load(file)\n",
    "\n",
    "with open(additional_set_path, \"rb\") as file:\n",
    "    additional_feature_set = pickle.load(file)\n",
    "\n",
    "print(f\"original feature set\")\n",
    "print(f\"    number of samples : {len(original_feature_set['19'])}\")\n",
    "print(f\"    number of features: {len(original_feature_set['19'][0])}\")\n",
    "\n",
    "print(f\"additional feature set\")\n",
    "print(f\"    number of samples : {len(additional_feature_set['19'])}\")\n",
    "print(f\"    number of features: {len(additional_feature_set['19'][0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new feature set\n",
      "    number of samples : 300\n",
      "    number of features: 146\n",
      "1st original: -345.1817626953125\n",
      "1st new     : -345.1817626953125\n",
      "last original: 468.4084049203264\n",
      "additional   : [1.75809862e+03 1.81810197e+03 2.92269196e-02 3.68823242e+03]\n",
      "last 5 new   : [4.68408405e+02 1.75809862e+03 1.81810197e+03 2.92269196e-02\n",
      " 3.68823242e+03] \n"
     ]
    }
   ],
   "source": [
    "new_feature_set = add_arrays_dicts(original_feature_set, additional_feature_set)\n",
    "\n",
    "print(f\"new feature set\")\n",
    "print(f\"    number of samples : {len(new_feature_set['19'])}\")\n",
    "print(f\"    number of features: {len(new_feature_set['19'][0])}\")\n",
    "\n",
    "\n",
    "print(f\"1st original: {original_feature_set['19'][0][0]}\")\n",
    "print(f\"1st new     : {new_feature_set['19'][0][0]}\")\n",
    "\n",
    "print(f\"last original: {original_feature_set['19'][0][-1]}\")\n",
    "print(f\"additional   : {additional_feature_set['19'][0]}\")\n",
    "print(f\"last 5 new   : {new_feature_set['19'][0][-5:]} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"../../../data/extracted_features_v2/mfcc_20_format_freq.pickle\", \"wb\") as file:\n",
    "    pickle.dump(new_feature_set, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original feature set\n",
      "    number of samples : 300\n",
      "    number of features: 140\n",
      "additional feature set\n",
      "    number of samples : 300\n",
      "    number of features: 142\n"
     ]
    }
   ],
   "source": [
    "original_set_path   = \"../../../data/extracted_features_v2/lfcc_features.pickle\"\n",
    "additional_set_path = \"../../../data/extracted_features_v2/mfcc_20_features.pickle\"\n",
    "\n",
    "with open(original_set_path, \"rb\") as file:\n",
    "    original_feature_set = pickle.load(file)\n",
    "\n",
    "with open(additional_set_path, \"rb\") as file:\n",
    "    additional_feature_set = pickle.load(file)\n",
    "\n",
    "print(f\"original feature set\")\n",
    "print(f\"    number of samples : {len(original_feature_set['19'])}\")\n",
    "print(f\"    number of features: {len(original_feature_set['19'][0])}\")\n",
    "\n",
    "print(f\"additional feature set\")\n",
    "print(f\"    number of samples : {len(additional_feature_set['19'])}\")\n",
    "print(f\"    number of features: {len(additional_feature_set['19'][0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new feature set\n",
      "    number of samples : 300\n",
      "    number of features: 142\n",
      "1st original: 6.589507102966309\n",
      "1st new     : 6.589507102966309\n",
      "last original: -2.663179397583008\n",
      "additional   : [514.873637   468.40840492]\n",
      "last 5 new   : [ -3.66989946  -3.18244052  -2.6631794  514.873637   468.40840492] \n"
     ]
    }
   ],
   "source": [
    "new_feature_set = add_last_n_elements(original_feature_set, additional_feature_set, 2)\n",
    "\n",
    "print(f\"new feature set\")\n",
    "print(f\"    number of samples : {len(new_feature_set['19'])}\")\n",
    "print(f\"    number of features: {len(new_feature_set['19'][0])}\")\n",
    "\n",
    "\n",
    "print(f\"1st original: {original_feature_set['19'][0][0]}\")\n",
    "print(f\"1st new     : {new_feature_set['19'][0][0]}\")\n",
    "\n",
    "print(f\"last original: {original_feature_set['19'][0][-1]}\")\n",
    "print(f\"additional   : {additional_feature_set['19'][0][-2:]}\")\n",
    "print(f\"last 5 new   : {new_feature_set['19'][0][-5:]} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"../../../data/extracted_features_v2/lfcc_20_pitch.pickle\", \"wb\") as file:\n",
    "    pickle.dump(new_feature_set, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BioMed_Project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
